{"cells":[{"cell_type":"markdown","metadata":{"id":"b0Yf4NBJUSNM"},"source":["# Створення нейронної мережі\n","\n","У цьому завданні ми створимо повнозв'язну нейронну мережу, використовуючи при цьому низькорівневі механізми tensorflow.\n","\n","Архітектура нейромережі представлена на наступному малюнку. Як бачиш, у ній є один вхідний шар, два приховані, а також вихідний шар. В якості активаційної функції у прихованих шарах буде використовуватись сигмоїда. На вихідному шарі ми використовуємо softmax.\n","\n","Частина коду зі створення мережі вже написана, тобі потрібно заповнити пропуски у вказаних місцях."]},{"cell_type":"markdown","metadata":{"id":"01rZWUu0USNQ"},"source":["## Архітектура нейронної мережі\n","\n","<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"nn\" style=\"width: 400px;\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"LLvIZ705Qw_V"},"source":["## Про датасет MNIST\n","\n","Дану нейромережу ми будемо вивчати на датасеті MNIST. Цей датасет являє собою велику кількість зображень рукописних цифр розміром $28 \\times 28$ пікселів. Кожен піксель приймає значення від 0 до 255.\n","\n","Як і раніше, датасет буде розділений на навчальну та тестову вибірки. При цьому ми виконаємо нормалізацію всіх зображень, щоб значення пікселів знаходилось у проміжку від 0 до 1, розділивши яскравість кожного пікселя на 255.\n","\n","Окрім того, архітектура нейронної мережі очікує на вхід вектор. У нашому ж випадку кожен об'єкт вибірки являє собою матрицю. Що ж робити? У цьому завданні ми \"розтягнемо\" матрицю $28 \\times 28$, отримавши при цьому вектор, що складається з 784 елементів.\n","\n","![MNIST Dataset](https://www.researchgate.net/profile/Steven-Young-5/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset.png)\n","\n","Більше інформації про датасет можна знайти [тут](http://yann.lecun.com/exdb/mnist/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"il_0_5OyUSNR"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import random\n","import keras as K\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cd-1_abTUSNS"},"outputs":[],"source":["num_classes = 10 # загальна кількість класів, у нашому випадку це цифри від 0 до 9\n","num_features = 784 # кількість атрибутів вхідного вектора 28 * 28 = 784\n","\n","learning_rate = 0.001 # швидкість навчання нейронної мережі\n","training_steps = 3000 # максимальне число епох\n","batch_size = 256 # перераховувати ваги мережі ми будемо не на всій вибірці, а на її випадковій підмножині з batch_size елементів\n","display_step = 100 # кожні 100 ітерацій ми будемо показувати поточне значення функції втрат і точності\n","\n","n_hidden_1 = 128 # кількість нейронів 1-го шару\n","n_hidden_2 = 256 # кількість нейронів 2-го шару"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pGTXiRyTUSNT","outputId":"7548ce3a-2c73-472f-e2b1-6ed25a170755","executionInfo":{"status":"ok","timestamp":1705299130233,"user_tz":480,"elapsed":1578,"user":{"displayName":"Yuriy Nikitchenko","userId":"06424287794076248218"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]}],"source":["# from tensorflow.keras.datasets import mnist\n","from keras.datasets import mnist\n","\n","# Завантажуємо датасет\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Перетворюємо цілочисельні пікселі на тип float32\n","x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n","\n","# Перетворюємо матриці розміром 28x28 пікселів у вектор з 784 елементів\n","x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n","\n","# Нормалізуємо значення пікселів\n","x_train, x_test = x_train / 255., x_test / 255.\n","\n","# Перемішаємо тренувальні дані\n","train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkRmCQjnUSNV"},"outputs":[],"source":["# Створимо нейронну мережу\n","\n","class DenseLayer(tf.Module):\n","    def __init__(self, in_features, out_features, name=None):\n","        super().__init__(name=name)\n","        self.w = tf.Variable(\n","            tf.random.normal([in_features, out_features]), name=\"w\"\n","        )\n","        self.b = tf.Variable(tf.zeros([out_features]), name=\"b\")\n","\n","    def __call__(self, x, activation=0):\n","        y = tf.matmul(x, self.w) + self.b\n","        if activation != 0:\n","            return tf.nn.softmax(y)\n","        else:\n","            return tf.nn.sigmoid(y)"]},{"cell_type":"code","source":["class NN(tf.Module):\n","  def __init__(self, name=None):\n","    super().__init__(name=name)\n","    # Перший шар, який складається з 128 нейронів\n","    self.layer_1 = DenseLayer(in_features=num_features, out_features=n_hidden_1)\n","    # Другий шар, який складається з 256 нейронів\n","    self.layer_2 = DenseLayer(in_features=n_hidden_1, out_features=n_hidden_2)\n","    # Вихідний шар\n","    self.layer_3 = DenseLayer(in_features=n_hidden_2, out_features=1)\n","\n","\n","  def __call__(self, x):\n","    # Передача даних через перші два шари та вихідний шар з функцією активації softmax\n","    x = self.layer_1(x)\n","    x = self.layer_2(x)\n","    return self.layer_3(x, 1)\n"],"metadata":{"id":"9wYXAA1EhVVJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from keras.layers import Input, Dense, BatchNormalization\n","# from keras import Sequential\n","\n","# model = Sequential(\n","#     [\n","#         Input(shape=(28,28)),\n","#         Dense(128, activation='sigmoid'),\n","#         Dense(256, activation='sigmoid'),\n","#         Dense(10, activation='softmax')\n","#     ]\n","# )\n"],"metadata":{"id":"Uqk9cjpvudAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIf3o7VAUSNV"},"outputs":[],"source":["# В якості функції помилки в даному випадку зручно взяти крос-ентропію\n","def cross_entropy(y_pred, y_true):\n","    # Закодувати label в one hot vector\n","    y_true = tf.one_hot(y_true, depth=num_classes)\n","\n","    # Значення передбачення, щоб уникнути помилки log(0).\n","    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n","\n","    # Обчислення крос-ентропії\n","    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n","\n","# Як метрику якості використовуємо точність\n","def accuracy(y_pred, y_true):\n","    # tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    # return (tp + tn) / (tp + tn + fp + fn)\n","    return accuracy_score(y_true, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQeT1yatUSNW"},"outputs":[],"source":["# Створимо екземпляр нейронної мережі\n","neural_net = NN(name=\"mnist\")\n","\n","# Функція навчання нейромережі\n","def train(neural_net, input_x, output_y):\n","  # Для налаштування вагів мережі будемо використовувати стохастичний градієнтний спуск\n","  optimizer = tf.optimizers.SGD(learning_rate)\n","\n","  # Активація автоматичного диференціювання\n","  with tf.GradientTape() as g:\n","    pred = neural_net(input_x)\n","    loss = cross_entropy(pred, output_y)\n","\n","    # Отримаємо список оптимізованих параметрів   ?????\n","    w1 = neural_net.layer_1.w\n","    b1 = neural_net.layer_1.b\n","    w2 = neural_net.layer_2.w\n","    b2 = neural_net.layer_2.b\n","    w3 = neural_net.layer_3.w\n","    b3 = neural_net.layer_3.b\n","\n","    # Обчислимо за ними значення градієнта   ?????\n","    dw3, db3 = g.gradient(loss, [w3, b3])\n","    dw2, db2 = g.gradient(loss, [w2, b2])\n","    dw1, db1 = g.gradient(loss, [w1, b1])\n","\n","    # Модифікуємо параметри   ?????\n","    neural_net.layer_3.w.assign_sub(learning_rate * dw3)\n","    neural_net.layer_3.b.assign_sub(learning_rate * db3)\n","\n","    neural_net.layer_2.w.assign_sub(learning_rate * dw2)\n","    neural_net.layer_2.b.assign_sub(learning_rate * db2)\n","\n","    neural_net.layer_1.w.assign_sub(learning_rate * dw1)\n","    neural_net.layer_1.b.assign_sub(learning_rate * db1)\n"]},{"cell_type":"code","source":["# model.compile(\n","#     optimizer=tf.keras.optimizers.SGD(learning_rate),\n","#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","#     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n","# )"],"metadata":{"id":"4dWmapX5xIfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fnyns9lBfpQZ"},"outputs":[],"source":["# Тренування мережі\n","\n","loss_history = []  # кожні display_step кроків зберігай в цьому списку поточну помилку нейромережі\n","accuracy_history = [] # кожні display_step кроків зберігай в цьому списку поточну точність нейромережі\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# У цьому циклі ми будемо проводити навчання нейронної мережі\n","# із тренувального датасета train_data вилучи випадкову підмножину, на якій\n","# відбудеться тренування. Використовуй метод take, доступний для тренувального датасета.\n","for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","    # Оновлюємо ваги нейронної мережі\n","    # train(neural_net, batch_x, batch_y)\n","\n","    with tf.GradientTape() as tape:\n","        logits = model(batch_x, training=True)\n","        loss_value = loss_fn(batch_y, logits)\n","\n","    grads = tape.gradient(loss_value, model.trainable_weights)\n","    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","    if step % display_step == 0:\n","        pred = neural_net(batch_x)\n","        loss = cross_entropy(pred, batch_y)\n","        acc = accuracy(pred, batch_y)\n","        loss_history.append(loss)\n","        accuracy_history.append(acc)\n"]},{"cell_type":"code","source":["# # Тренування мережі\n","\n","# loss_history = []  # кожні display_step кроків зберігай в цьому списку поточну помилку нейромережі\n","# accuracy_history = [] # кожні display_step кроків зберігай в цьому списку поточну точність нейромережі\n","\n","# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n","# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# # У цьому циклі ми будемо проводити навчання нейронної мережі\n","# # із тренувального датасета train_data вилучи випадкову підмножину, на якій\n","# # відбудеться тренування. Використовуй метод take, доступний для тренувального датасета.\n","# for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","#     # Оновлюємо ваги нейронної мережі\n","\n","#     with tf.GradientTape() as tape:\n","#         logits = model(batch_x, training=True)\n","#         loss_value = loss_fn(batch_y, logits)\n","\n","#     grads = tape.gradient(loss_value, model.trainable_weights)\n","#     optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","\n","#     if step % display_step == 0:\n","#         loss_history.append(loss_value)\n"],"metadata":{"id":"gv8WqrHBYgZn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yCBfG6MbQB2"},"outputs":[],"source":["# Виведіть графіки залежності зміни точності і втрат від кроку\n","# Якщо все зроблено правильно, то точність повинна зростати, а втрати зменшуватись\n","\n","import matplotlib.pyplot as plt\n","\n","# Виведіть графік функції втрат\n","# Місце для вашого коду\n","\n","# Виведіть графік точності\n","# Місце для вашого коду\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LE3g4gDyUSNY"},"outputs":[],"source":["# Обчисліть точність навченої нейромережі\n","# Місце для вашого коду\n","# Тестування моделі на тестових даних\n","# Місце для вашого коду"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EEHAubOUSNY"},"outputs":[],"source":["# Протестуйте навчену нейромережу на 10 зображеннях. З тестової вибірки візьміть 5\n","# випадкових зображень і передайте їх у нейронну мережу.\n","# Виведіть зображення та випишіть  поруч відповідь нейромережі.\n","# Зробіть висновок про те, чи помиляється твоя нейронна мережа, і якщо так, то як часто?\n","\n","# Місце для вашого коду\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"nbformat":4,"nbformat_minor":0}